{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e905a588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59c66630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Read the CSV dataframe\n",
    "df = pd.read_csv(\"Data/corrected_data.csv\")\n",
    "\n",
    "\n",
    "\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "\n",
    "# df = df.sample(frac=0.1)\n",
    "# Split the dataframe into train, val, and test splits\n",
    "train_df = df.sample(frac=0.8)\n",
    "val_df = df.drop(train_df.index)\n",
    "test_df = val_df.sample(frac=0.5)\n",
    "\n",
    "# Create the hugging face dataset\n",
    "dataset = DatasetDict()\n",
    "dataset[\"train\"] = Dataset.from_pandas(train_df)\n",
    "dataset[\"val\"] = Dataset.from_pandas(val_df)\n",
    "dataset[\"test\"] = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d736859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-multilingual-cased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06deb339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "    max_length = 128\n",
    "    return tokenizer(\n",
    "        examples[\"Content\"], truncation=True, padding=\"max_length\", max_length=max_length\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76bd6c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30428 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7607 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3804 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ac49116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ee99dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Content', 'labels', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 30428\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['Content', 'labels', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 7607\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Content', 'labels', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 3804\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "694c5d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28a9f8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def objective(model, weight_decay, num_train_epochs, train_batch, eval_batch, optimizer, lr_scheduler):\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "    model.to(device)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"Distil-test-exp4/\",\n",
    "        weight_decay=weight_decay,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=train_batch,\n",
    "        per_device_eval_batch_size=eval_batch,\n",
    "#         disable_tqdm=True,\n",
    "        report_to = 'wandb'\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"val\"],\n",
    "        optimizers = (optimizer,lr_scheduler),\n",
    "    )\n",
    "\n",
    "    wandb.init(project=\"Unmasking-Hate\", name=\"distilbert-tel-run-custom-4\")\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate the model after each epoch.\n",
    "    progress_bar = tqdm(range(training_args.num_train_epochs), desc=\"Epochs\", position=0)\n",
    "    for epoch in progress_bar:\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_targets = []\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        for batch in trainer.get_eval_dataloader():\n",
    "            model_input = batch[\"input_ids\"].to(trainer.model.device)\n",
    "            targets = batch[\"labels\"].to(trainer.model.device)\n",
    "            \n",
    "            outputs = trainer.model(model_input, labels=targets)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_predictions.append(logits.detach().cpu().numpy())\n",
    "            train_targets.append(targets.detach().cpu().numpy())\n",
    "            \n",
    "        train_loss /= len(trainer.get_eval_dataloader())\n",
    "        train_predictions = np.concatenate(train_predictions, axis=0)\n",
    "        train_targets = np.concatenate(train_targets, axis=0)\n",
    "        train_accuracy = (train_predictions.argmax(axis=1) == train_targets).mean()\n",
    "        \n",
    "        val_loss = trainer.evaluate().get(\"eval_loss\", None)\n",
    "        \n",
    "        # Call trainer.predict() on the val dataset\n",
    "        val_outputs = trainer.predict(dataset[\"val\"])\n",
    "        val_predictions = val_outputs.predictions\n",
    "        val_targets = val_outputs.label_ids\n",
    "            \n",
    "        val_accuracy = (val_predictions.argmax(axis=1) == val_targets).mean()\n",
    "        \n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            val_targets, val_predictions.argmax(axis=1), average=\"weighted\"\n",
    "        )\n",
    "    \n",
    "        print(f\"Epoch: {epoch + 1}\",f\"Train Loss: {train_loss}\",f\"Val Loss: {val_loss}\",f\"Train Accuracy: {train_accuracy}\",f\"Val Accuracy: {val_accuracy}\",f\"Precision: {precision}\",f\"Recall: {recall}\",f\"F1: {f1}\")\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"Epoch\": epoch + 1,\n",
    "                \"Train Loss\": train_loss,\n",
    "                \"Val Loss\": val_loss,\n",
    "                \"Train Accuracy\": train_accuracy,\n",
    "                \"Val Accuracy\": val_accuracy,\n",
    "                \"Precision\": precision,\n",
    "                \"Recall\": recall,\n",
    "                \"F1\": f1,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0943ca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c688bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msiddarthakoppaka\u001b[0m (\u001b[33munmask-hate\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7325d64bbfae46d688b5359ac602299b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\Hate Speech Recognition\\wandb\\run-20230517_222946-vt17eesb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unmask-hate/Unmasking-Hate/runs/vt17eesb' target=\"_blank\">distilbert-tel-run-custom-4</a></strong> to <a href='https://wandb.ai/unmask-hate/Unmasking-Hate' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unmask-hate/Unmasking-Hate' target=\"_blank\">https://wandb.ai/unmask-hate/Unmasking-Hate</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unmask-hate/Unmasking-Hate/runs/vt17eesb' target=\"_blank\">https://wandb.ai/unmask-hate/Unmasking-Hate/runs/vt17eesb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22824' max='22824' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22824/22824 1:46:42, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.663400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.664700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.660900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.662700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.660500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.660600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.661100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.661500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.660300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.660300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.663000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.659700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.662100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.663000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.659900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.661200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.663800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.661000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.662200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.662300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.660200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.659000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.661100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.662200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.659700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.661100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.663500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.661300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.660600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.660200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.666300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.660900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.661200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.662100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.659200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.660700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.662100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.662200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.661500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.659300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epochs:   0%|                                                                                    | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epochs:  17%|████████████▌                                                              | 1/6 [02:50<14:13, 170.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 0.6853013901680426 Val Loss: 0.654646635055542 Train Accuracy: 0.5370053897725778 Val Accuracy: 0.6696463783357434 Precision: 0.7457811503630998 Recall: 0.6696463783357434 F1: 0.6337381181427203\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epochs:  33%|█████████████████████████                                                  | 2/6 [05:39<11:19, 169.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train Loss: 0.683364329528608 Val Loss: 0.654646635055542 Train Accuracy: 0.5253056395425266 Val Accuracy: 0.6696463783357434 Precision: 0.7457811503630998 Recall: 0.6696463783357434 F1: 0.6337381181427203\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epochs:  50%|█████████████████████████████████████▌                                     | 3/6 [08:29<08:29, 169.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train Loss: 0.683364329528608 Val Loss: 0.654646635055542 Train Accuracy: 0.5253056395425266 Val Accuracy: 0.6696463783357434 Precision: 0.7457811503630998 Recall: 0.6696463783357434 F1: 0.6337381181427203\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epochs:  67%|██████████████████████████████████████████████████                         | 4/6 [11:18<05:39, 169.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train Loss: 0.683364329528608 Val Loss: 0.654646635055542 Train Accuracy: 0.5253056395425266 Val Accuracy: 0.6696463783357434 Precision: 0.7457811503630998 Recall: 0.6696463783357434 F1: 0.6337381181427203\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epochs:  83%|██████████████████████████████████████████████████████████████▌            | 5/6 [14:08<02:49, 169.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train Loss: 0.683364329528608 Val Loss: 0.654646635055542 Train Accuracy: 0.5253056395425266 Val Accuracy: 0.6696463783357434 Precision: 0.7457811503630998 Recall: 0.6696463783357434 F1: 0.6337381181427203\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|███████████████████████████████████████████████████████████████████████████| 6/6 [16:57<00:00, 169.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Train Loss: 0.683364329528608 Val Loss: 0.654646635055542 Train Accuracy: 0.5253056395425266 Val Accuracy: 0.6696463783357434 Precision: 0.7457811503630998 Recall: 0.6696463783357434 F1: 0.6337381181427203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>F1</td><td>▁▁▁▁▁▁</td></tr><tr><td>Precision</td><td>▁▁▁▁▁▁</td></tr><tr><td>Recall</td><td>▁▁▁▁▁▁</td></tr><tr><td>Train Accuracy</td><td>█▁▁▁▁▁</td></tr><tr><td>Train Loss</td><td>█▁▁▁▁▁</td></tr><tr><td>Val Accuracy</td><td>▁▁▁▁▁▁</td></tr><tr><td>Val Loss</td><td>▁▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▂▄▅▁▆█</td></tr><tr><td>eval/samples_per_second</td><td>▇▅▄█▃▁</td></tr><tr><td>eval/steps_per_second</td><td>▇▅▄█▃▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███████████</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▆▇▃▅▃▃▄▄▃▅▂▄▅▃▄▆▅▅▃▂▄▅▂▄▄▆▄▄▃▃█▃▁▄▂▃▄▃▅▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>6</td></tr><tr><td>F1</td><td>0.63374</td></tr><tr><td>Precision</td><td>0.74578</td></tr><tr><td>Recall</td><td>0.66965</td></tr><tr><td>Train Accuracy</td><td>0.52531</td></tr><tr><td>Train Loss</td><td>0.68336</td></tr><tr><td>Val Accuracy</td><td>0.66965</td></tr><tr><td>Val Loss</td><td>0.65465</td></tr><tr><td>eval/loss</td><td>0.65465</td></tr><tr><td>eval/runtime</td><td>53.6696</td></tr><tr><td>eval/samples_per_second</td><td>141.738</td></tr><tr><td>eval/steps_per_second</td><td>17.72</td></tr><tr><td>train/epoch</td><td>6.0</td></tr><tr><td>train/global_step</td><td>22824</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6593</td></tr><tr><td>train/total_flos</td><td>6046077009457152.0</td></tr><tr><td>train/train_loss</td><td>0.66137</td></tr><tr><td>train/train_runtime</td><td>6403.4308</td></tr><tr><td>train/train_samples_per_second</td><td>28.511</td></tr><tr><td>train/train_steps_per_second</td><td>3.564</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">distilbert-tel-run-custom-4</strong> at: <a href='https://wandb.ai/unmask-hate/Unmasking-Hate/runs/vt17eesb' target=\"_blank\">https://wandb.ai/unmask-hate/Unmasking-Hate/runs/vt17eesb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230517_222946-vt17eesb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "optimizer = AdamW(model.parameters(), lr=0.0001)\n",
    "num_training_steps = 31.25\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*num_training_steps, num_training_steps=num_training_steps)\n",
    "objective(model,0.0001,6,8,8, optimizer, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef1d7a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('Distil_exp4/Distil_hf/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4c23d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2700c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights\n",
    "torch.save(model.state_dict(), \"Distil_exp4/distil-model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a452ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
